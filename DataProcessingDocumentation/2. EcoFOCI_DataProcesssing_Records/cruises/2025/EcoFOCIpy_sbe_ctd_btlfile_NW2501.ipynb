{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "educational-chosen",
   "metadata": {},
   "source": [
    "# Using EcoFOCIpy to process raw field data\n",
    "\n",
    "## NW2501 (NBS Survey - NorthWest Explorer)\n",
    "\n",
    "## CTD / BTL Data\n",
    "\n",
    "Basic workflow for each instrument grouping is *(initial archive level)*:\n",
    "- SBE workflow must happen first\n",
    "- Parse data from btl files into pandas dataframe\n",
    "\n",
    "Convert to xarray dataframe for all following work *(working or final data level):\n",
    "- Add metadata from cruise yaml files and/or header info\n",
    "- ingest metadata from cruise / cast logs\n",
    "- process data beyond simple file translate\n",
    "- apply any calibrations or corrections\n",
    "    + field corrections\n",
    "    + offsets\n",
    "    + instrument compensations\n",
    "    + some QC were available... this would be old-school simple bounds mostly\n",
    "- adjust time bounds and sample frequency (xarray dataframe)\n",
    "- save as CF netcdf via xarray: so many of the steps above are optional\n",
    "    + **ERDDAP NRT** if no corrections, offsets or time bounds are applied but some meta data is\n",
    "    + **Working and awaiting QC** has no ERDDAP representation and is a holding spot\n",
    "    + **ERDDAP Final** fully calibrated, qc'd and populated with meta information\n",
    "\n",
    "Plot for preview and QC\n",
    "- preview images (indiv and/or collectively)\n",
    "- manual qc process\n",
    "- automated qc process ML/AI\n",
    "\n",
    "Further refinenments for ERDDAP hosting:\n",
    "\n",
    "## Differences from EPIC standard\n",
    "- previously btl files had coordinates of lat/lon/time/depth...with a bottle position / fireing order being a variable, this means that if multiple bottles are fired at the same depth, the file was not uniquely indexed and the bottle variable had to be conflated for multiple fireings\n",
    "- in new version, we will index based on lat/lon/time/bottle_num (bottle number is a sequential unique value... often representing the position on the rosette), merging with CTD downcast data will require maintining a pressure/depth variable in the bottle data that can be rounded to the nearest 1m bin.  This does not solve the problem with multiple discrete samples taken at a single depth and single niskin though."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-miniature",
   "metadata": {},
   "source": [
    "## Example below is for SBE 9/11+ V2 but the workflow is similar for any SBE instruments.\n",
    "\n",
    "Future processing of this instrument can be a simplified (no markdown) process which can be archived so that the procedure can be traced or updated\n",
    "\n",
    "We process each cast as an individual file so this example will not loop over all profiles.  See `example/all_casts.py` example for processing an entire cruise at once.\n",
    "\n",
    "Adding Discrete samples such as Oxygen, Chlorophyll, Salinity to BTL Data is in `example/discrete_castdata.py`.  Its purpose is to match niskin/bottle information to depth for the discrete data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "studied-pollution",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import glob\n",
    "\n",
    "import EcoFOCIpy.io.sbe_ctd_parser as sbe_ctd_parser #<- instrument specific\n",
    "import EcoFOCIpy.io.ncCFsave as ncCFsave\n",
    "import EcoFOCIpy.metaconfig.load_config as load_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-millennium",
   "metadata": {},
   "source": [
    "The sample_data_dir should be included in the github package but may not be included in the pip install of the package\n",
    "\n",
    "## Simple Processing - first step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "offensive-level",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_dir = '/Users/bell/ecoraid/2025/CTDcasts/nw2501/' #root path to cruise directory\n",
    "ecofocipy_dir = '/Users/bell/Programs/EcoFOCIpy/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "third-yellow",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/bell/ecoraid/2025/CTDcasts/nw2501/rawconverted/ctd001.btl\n",
      "Processing /Users/bell/ecoraid/2025/CTDcasts/nw2501/rawconverted/ctd002.btl\n",
      "Processing /Users/bell/ecoraid/2025/CTDcasts/nw2501/rawconverted/ctd003.btl\n",
      "Processing /Users/bell/ecoraid/2025/CTDcasts/nw2501/rawconverted/ctd004.btl\n",
      "Processing /Users/bell/ecoraid/2025/CTDcasts/nw2501/rawconverted/ctd005.btl\n",
      "Processing /Users/bell/ecoraid/2025/CTDcasts/nw2501/rawconverted/ctd006.btl\n",
      "Processing /Users/bell/ecoraid/2025/CTDcasts/nw2501/rawconverted/ctd007.btl\n",
      "Processing /Users/bell/ecoraid/2025/CTDcasts/nw2501/rawconverted/ctd008.btl\n",
      "Processing /Users/bell/ecoraid/2025/CTDcasts/nw2501/rawconverted/ctd009.btl\n",
      "Processing /Users/bell/ecoraid/2025/CTDcasts/nw2501/rawconverted/ctd010.btl\n",
      "Processing /Users/bell/ecoraid/2025/CTDcasts/nw2501/rawconverted/ctd011.btl\n",
      "Processing /Users/bell/ecoraid/2025/CTDcasts/nw2501/rawconverted/ctd012.btl\n",
      "Processing /Users/bell/ecoraid/2025/CTDcasts/nw2501/rawconverted/ctd013.btl\n",
      "Processing /Users/bell/ecoraid/2025/CTDcasts/nw2501/rawconverted/ctd014.btl\n",
      "Processing /Users/bell/ecoraid/2025/CTDcasts/nw2501/rawconverted/ctd015.btl\n",
      "Processing /Users/bell/ecoraid/2025/CTDcasts/nw2501/rawconverted/ctd016.btl\n",
      "Processing /Users/bell/ecoraid/2025/CTDcasts/nw2501/rawconverted/ctd017.btl\n",
      "Processing /Users/bell/ecoraid/2025/CTDcasts/nw2501/rawconverted/ctd018.btl\n",
      "Processing /Users/bell/ecoraid/2025/CTDcasts/nw2501/rawconverted/ctd019.btl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bell/src/ecofocipy/src/EcoFOCIpy/io/sbe_ctd_parser.py:91: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  ctd_df = pd.concat([ctd_df,row])\n",
      "/Users/bell/src/ecofocipy/src/EcoFOCIpy/io/sbe_ctd_parser.py:91: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  ctd_df = pd.concat([ctd_df,row])\n",
      "/Users/bell/src/ecofocipy/src/EcoFOCIpy/io/sbe_ctd_parser.py:91: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  ctd_df = pd.concat([ctd_df,row])\n",
      "/Users/bell/src/ecofocipy/src/EcoFOCIpy/io/sbe_ctd_parser.py:91: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  ctd_df = pd.concat([ctd_df,row])\n",
      "/Users/bell/src/ecofocipy/src/EcoFOCIpy/io/sbe_ctd_parser.py:91: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  ctd_df = pd.concat([ctd_df,row])\n",
      "/Users/bell/src/ecofocipy/src/EcoFOCIpy/io/sbe_ctd_parser.py:91: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  ctd_df = pd.concat([ctd_df,row])\n",
      "/Users/bell/src/ecofocipy/src/EcoFOCIpy/io/sbe_ctd_parser.py:91: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  ctd_df = pd.concat([ctd_df,row])\n",
      "/Users/bell/src/ecofocipy/src/EcoFOCIpy/io/sbe_ctd_parser.py:91: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  ctd_df = pd.concat([ctd_df,row])\n",
      "/Users/bell/src/ecofocipy/src/EcoFOCIpy/io/sbe_ctd_parser.py:91: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  ctd_df = pd.concat([ctd_df,row])\n",
      "/Users/bell/src/ecofocipy/src/EcoFOCIpy/io/sbe_ctd_parser.py:91: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  ctd_df = pd.concat([ctd_df,row])\n",
      "/Users/bell/src/ecofocipy/src/EcoFOCIpy/io/sbe_ctd_parser.py:91: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  ctd_df = pd.concat([ctd_df,row])\n",
      "/Users/bell/src/ecofocipy/src/EcoFOCIpy/io/sbe_ctd_parser.py:91: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  ctd_df = pd.concat([ctd_df,row])\n",
      "/Users/bell/src/ecofocipy/src/EcoFOCIpy/io/sbe_ctd_parser.py:91: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  ctd_df = pd.concat([ctd_df,row])\n",
      "/Users/bell/src/ecofocipy/src/EcoFOCIpy/io/sbe_ctd_parser.py:91: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  ctd_df = pd.concat([ctd_df,row])\n",
      "/Users/bell/src/ecofocipy/src/EcoFOCIpy/io/sbe_ctd_parser.py:91: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  ctd_df = pd.concat([ctd_df,row])\n",
      "/Users/bell/src/ecofocipy/src/EcoFOCIpy/io/sbe_ctd_parser.py:91: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  ctd_df = pd.concat([ctd_df,row])\n",
      "/Users/bell/src/ecofocipy/src/EcoFOCIpy/io/sbe_ctd_parser.py:91: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  ctd_df = pd.concat([ctd_df,row])\n",
      "/Users/bell/src/ecofocipy/src/EcoFOCIpy/io/sbe_ctd_parser.py:91: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  ctd_df = pd.concat([ctd_df,row])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "20 columns passed, passed data had 21 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mambaforge/envs/p312/lib/python3.12/site-packages/pandas/core/internals/construction.py:939\u001b[39m, in \u001b[36m_finalize_columns_and_data\u001b[39m\u001b[34m(content, columns, dtype)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m939\u001b[39m     columns = \u001b[43m_validate_or_indexify_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    941\u001b[39m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mambaforge/envs/p312/lib/python3.12/site-packages/pandas/core/internals/construction.py:986\u001b[39m, in \u001b[36m_validate_or_indexify_columns\u001b[39m\u001b[34m(content, columns)\u001b[39m\n\u001b[32m    984\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mi_list \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) != \u001b[38;5;28mlen\u001b[39m(content):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m    985\u001b[39m     \u001b[38;5;66;03m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m986\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    987\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m columns passed, passed data had \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    988\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m columns\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    989\u001b[39m     )\n\u001b[32m    990\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_mi_list:\n\u001b[32m    991\u001b[39m     \u001b[38;5;66;03m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[31mAssertionError\u001b[39m: 20 columns passed, passed data had 21 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m cruise = sbe_ctd_parser.sbe_btl()\n\u001b[32m     12\u001b[39m filename_list = \u001b[38;5;28msorted\u001b[39m(glob.glob(datafile + \u001b[33m'\u001b[39m\u001b[33m*.btl\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m cruise_data = \u001b[43mcruise\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmanual_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/ecofocipy/src/EcoFOCIpy/io/sbe_ctd_parser.py:86\u001b[39m, in \u001b[36msbe_btl.manual_parse\u001b[39m\u001b[34m(file_list)\u001b[39m\n\u001b[32m     82\u001b[39m     data = [data[\u001b[32m0\u001b[39m]]+[\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(data[\u001b[32m1\u001b[39m:\u001b[32m4\u001b[39m])]+data[\u001b[32m4\u001b[39m::]\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33msdev\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m line: \u001b[38;5;66;03m#needed for the time column only\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     row = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m+\u001b[49m\u001b[43m[\u001b[49m\u001b[43mline\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m row.columns:\n\u001b[32m     89\u001b[39m         row[c] = row[c].astype(\u001b[38;5;28mfloat\u001b[39m,errors=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mambaforge/envs/p312/lib/python3.12/site-packages/pandas/core/frame.py:855\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    853\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    854\u001b[39m         columns = ensure_index(columns)\n\u001b[32m--> \u001b[39m\u001b[32m855\u001b[39m     arrays, columns, index = \u001b[43mnested_data_to_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[32m    857\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[32m    858\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    861\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    863\u001b[39m     mgr = arrays_to_mgr(\n\u001b[32m    864\u001b[39m         arrays,\n\u001b[32m    865\u001b[39m         columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    868\u001b[39m         typ=manager,\n\u001b[32m    869\u001b[39m     )\n\u001b[32m    870\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mambaforge/envs/p312/lib/python3.12/site-packages/pandas/core/internals/construction.py:520\u001b[39m, in \u001b[36mnested_data_to_arrays\u001b[39m\u001b[34m(data, columns, index, dtype)\u001b[39m\n\u001b[32m    517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[32m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    518\u001b[39m     columns = ensure_index(data[\u001b[32m0\u001b[39m]._fields)\n\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m arrays, columns = \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    521\u001b[39m columns = ensure_index(columns)\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mambaforge/envs/p312/lib/python3.12/site-packages/pandas/core/internals/construction.py:845\u001b[39m, in \u001b[36mto_arrays\u001b[39m\u001b[34m(data, columns, dtype)\u001b[39m\n\u001b[32m    842\u001b[39m     data = [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[32m    843\u001b[39m     arr = _list_to_arrays(data)\n\u001b[32m--> \u001b[39m\u001b[32m845\u001b[39m content, columns = \u001b[43m_finalize_columns_and_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mambaforge/envs/p312/lib/python3.12/site-packages/pandas/core/internals/construction.py:942\u001b[39m, in \u001b[36m_finalize_columns_and_data\u001b[39m\u001b[34m(content, columns, dtype)\u001b[39m\n\u001b[32m    939\u001b[39m     columns = _validate_or_indexify_columns(contents, columns)\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    941\u001b[39m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    944\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[32m0\u001b[39m].dtype == np.object_:\n\u001b[32m    945\u001b[39m     contents = convert_object_array(contents, dtype=dtype)\n",
      "\u001b[31mValueError\u001b[39m: 20 columns passed, passed data had 21 columns"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "# edit to point to {cruise sepcific} raw datafiles \n",
    "datafile = sample_data_dir+'rawconverted/' #<- point to cruise and process all files within\n",
    "cruise_name = 'NW2501' #no hyphens\n",
    "cruise_meta_file = sample_data_dir+'logs/NW2501.yaml'\n",
    "inst_meta_file = sample_data_dir+'logs/EMA_standard_CTD.yaml' #<- copy to each deployment for simplicity?\n",
    "group_meta_file = ecofocipy_dir+'staticdata/institutional_meta_example.yaml'\n",
    "###############################################################\n",
    "\n",
    "#init and load data\n",
    "cruise = sbe_ctd_parser.sbe_btl()\n",
    "filename_list = sorted(glob.glob(datafile + '*.btl'))\n",
    "\n",
    "cruise_data = cruise.manual_parse(filename_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a034c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick statistical look at the distribution of data for a cast\n",
    "# #preview a dataframe\n",
    "cruise_data['ctd001.btl'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2b232e-a506-48e0-b01d-0e18674bd13e",
   "metadata": {},
   "source": [
    "## Create BTL report file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8687ee51-2edf-4252-901d-36394f4529e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# btl report file\n",
    "for cast in cruise_data.keys():\n",
    "    try:\n",
    "        df=cruise_data[cast]\n",
    "        df['cast'] = cast.lower().split('.')[0]\n",
    "        if cast.lower().split('.')[0] == 'ctd001':\n",
    "            df.to_csv(f'{cruise_name}.report_btl')\n",
    "        else:\n",
    "            df.to_csv(f'{cruise_name}.report_btl',mode='a', header=False)\n",
    "    except:\n",
    "        print(f'some issue in {cast}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-audit",
   "metadata": {},
   "source": [
    "## Time Properties\n",
    "\n",
    "Not traditionally dealt with for CTD files as they are likely dynamically updated via GPS feed.  However, FOCI tends to label the date/time with the ***at depth*** time-stamp\n",
    "\n",
    "## Depth Properties and other assumptions\n",
    "\n",
    "- currently, all processing and binning (1m for FOCI) is done via seabird routines and the windows software.  This may change with the python ctd package for a few tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-active",
   "metadata": {},
   "source": [
    "## Add Deployment meta information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-fairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#just a dictionary of dictionaries - simple\n",
    "with open(cruise_meta_file) as file:\n",
    "    cruise_config = yaml.full_load(file)\n",
    "cruise_config[cruise_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102dc1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#and if you want a cast from the cruise, just use the consective cast number\n",
    "cruise_config['CTDCasts']['CTD001']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-cornwall",
   "metadata": {},
   "source": [
    "## Add Instrument meta information\n",
    "\n",
    "Time, depth, lat, lon should be added regardless (always our coordinates) but for a mooring site its going to be a (1,1,1,t) dataset\n",
    "The variables of interest should be read from the data file and matched to a key for naming.  That key is in the inst_config file seen below and should represent common conversion names in the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-raise",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(inst_meta_file) as file:\n",
    "    inst_config = yaml.full_load(file)\n",
    "inst_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837d1466",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sbe data uses header info to name variables... but we want standard names from the dictionary I've created, so we need to rename column variables appropriately\n",
    "#rename values to appropriate names, if a value isn't in the .yaml file, you can add it\n",
    "\n",
    "#*** biggest *** difference between moored and profile data is there may be multiple instruments with the same dataype (e.g.) temperature\n",
    "# on the same platform.  We _used_ to use the phrases primary and secondary, but will now only refer to them as ch1, ch2 etc\n",
    "cruise_data['ctd001.btl'] = cruise_data['ctd001.btl'].rename(columns={\n",
    "                        't090c':'temperature_ch1',\n",
    "                        't190c':'temperature_ch2',\n",
    "                        'sal00':'salinity_ch1',\n",
    "                        'sal11':'salinity_ch2',\n",
    "                        'sbox0mm/kg':'oxy_conc_ch1',\n",
    "                        'sbeox0ml/l':'oxy_concM_ch1',\n",
    "                        'sbox1mm/kg':'oxy_conc_ch2',\n",
    "                        'sbeox1ml/l':'oxy_concM_ch2',\n",
    "                        'sbeox0ps':'oxy_percentsat_ch1',\n",
    "                        'sbeox1ps':'oxy_percentsat_ch2',\n",
    "                        'sigma-t00':'sigma_t_ch1',\n",
    "                        'sigma-t11':'sigma_t_ch2',\n",
    "                        'cstarat0':'Attenuation',\n",
    "                        'cstartr0':'Transmittance',\n",
    "                        'fleco-afl':'chlor_fluorescence',\n",
    "                        'turbwetntu0':'turbidity',\n",
    "                        'empty':'empty', #this will be ignored\n",
    "                        'prdm':'Pressure [dbar]',\n",
    "                        'flag':'flag'})\n",
    "\n",
    "cruise_data['ctd001.btl'].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15849d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "cruise_data['ctd001.btl'].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56119e2f",
   "metadata": {},
   "source": [
    "## Add institutional meta-information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b2c8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(group_meta_file) as file:\n",
    "    group_config = yaml.full_load(file)\n",
    "group_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-popularity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add meta data and prelim processing based on meta data\n",
    "# Convert to xarray and add meta information - save as CF netcdf file\n",
    "# pass -> data, instmeta, depmeta\n",
    "cruise_data_nc = ncCFsave.EcoFOCI_CFnc(df=cruise_data['ctd001.btl'], \n",
    "                                instrument_yaml=inst_config, \n",
    "                                operation_yaml=cruise_config,\n",
    "                                operation_type='ctd')\n",
    "cruise_data_nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0471a351",
   "metadata": {},
   "outputs": [],
   "source": [
    "cruise_data_nc.get_xdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-chambers",
   "metadata": {},
   "source": [
    "At this point, you could save your file with the `.xarray2netcdf_save()` method and have a functioning dataset.... but it would be very simple with no additional qc, meta-data, or tuned parameters for optimizing software like ferret or erddap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-pennsylvania",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand the dimensions and coordinate variables\n",
    "# renames them appropriatley and prepares them for meta-filled values\n",
    "cruise_data_nc.expand_dimensions(dim_names=['latitude','longitude','time'],geophys_sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-operations",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build list from columsn in data - if a variable isn't in the yaml file, it will be dropped from the final data fields\n",
    "cruise_data_nc.variable_meta_data(variable_keys=list(cruise_data['ctd001.btl'].columns.values),drop_missing=False)\n",
    "#adding dimension meta needs to come after updating the dimension values... BUG?\n",
    "cruise_data_nc.dimension_meta_data(variable_keys=['time','latitude','longitude'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-product",
   "metadata": {},
   "source": [
    "The following steps can happen in just about any order and are all meta-data driven.  Therefore, they are not required to have a functioning dataset, but they are required to have a well described dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00255625-1201-47e4-bdb2-cf1dcb8ae472",
   "metadata": {},
   "outputs": [],
   "source": [
    "cruise_data_nc.get_xdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-diversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add global attributes\n",
    "cruise_data_nc.deployment_meta_add(conscastno='CTD001')\n",
    "\n",
    "#add instituitonal global attributes\n",
    "cruise_data_nc.institution_meta_add(group_config)\n",
    "\n",
    "#add creation date/time - provenance data\n",
    "cruise_data_nc.provinance_meta_add()\n",
    "\n",
    "#provide intial qc status field\n",
    "cruise_data_nc.qc_status(qc_status='unqcd') #<- options are unknown, excellent, probably good, mixed, unqcd\n",
    "\n",
    "cruise_data_nc.get_xdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-hughes",
   "metadata": {},
   "source": [
    "## Save CF Netcdf files\n",
    "\n",
    "Currently stick to netcdf3 classic... but migrating to netcdf4 (default) may be no problems for most modern purposes.  Its easy enough to pass the `format` kwargs through to the netcdf api of xarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-volunteer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop over all casts and perform tasks shown above\n",
    "\n",
    "for cast in cruise_data.keys():\n",
    "    try:\n",
    "        cruise_data[cast] = cruise_data[cast].rename(columns={\n",
    "                            't090c':'temperature_ch1',\n",
    "                            't190c':'temperature_ch2',\n",
    "                            'sal00':'salinity_ch1',\n",
    "                            'sal11':'salinity_ch2',\n",
    "                            'sbox0mm/kg':'oxy_conc_ch1',\n",
    "                            'sbeox0ml/l':'oxy_concM_ch1',\n",
    "                            'sbox1mm/kg':'oxy_conc_ch2',\n",
    "                            'sbeox1ml/l':'oxy_concM_ch2',\n",
    "                            'sbeox0ps':'oxy_percentsat_ch1',\n",
    "                            'sbeox1ps':'oxy_percentsat_ch2',\n",
    "                            'sigma-t00':'sigma_t_ch1',\n",
    "                            'sigma-t11':'sigma_t_ch2',\n",
    "                            'cstarat0':'Attenuation',\n",
    "                            'cstartr0':'Transmittance',\n",
    "                            'fleco-afl':'chlor_fluorescence',\n",
    "                            'turbwetntu0':'turbidity',\n",
    "                            'prdm':'pressure',\n",
    "                            'empty':'empty', #this will be ignored\n",
    "                            'flag':'flag'})\n",
    "\n",
    "        cruise_data_nc = ncCFsave.EcoFOCI_CFnc(df=cruise_data[cast], \n",
    "                                    instrument_yaml=inst_config, \n",
    "                                    operation_yaml=cruise_config,\n",
    "                                    operation_type='ctd')\n",
    "\n",
    "        cruise_data_nc.expand_dimensions(dim_names=['latitude','longitude','time'],geophys_sort=False)\n",
    "\n",
    "        cruise_data_nc.variable_meta_data(variable_keys=list(cruise_data[cast].columns.values),drop_missing=True)\n",
    "        #adding dimension meta needs to come after updating the dimension values... BUG?\n",
    "        cruise_data_nc.dimension_meta_data(variable_keys=['time','latitude','longitude'])\n",
    "        cruise_data_nc.temporal_geospatioal_meta_data_ctd(positiveE=False,conscastno=cast.split('.')[0])\n",
    "\n",
    "        #add global attributes\n",
    "        cruise_data_nc.deployment_meta_add(conscastno=cast.split('.')[0].upper())\n",
    "\n",
    "        #add instituitonal global attributes\n",
    "        cruise_data_nc.institution_meta_add(group_config)\n",
    "\n",
    "        #add creation date/time - provenance data\n",
    "        cruise_data_nc.provinance_meta_add()\n",
    "\n",
    "        #provide intial qc status field\n",
    "        cruise_data_nc.qc_status(qc_status='unqcd') #<- options are unknown, excellent, probably good, mixed, unqcd\n",
    "\n",
    "        cast_label = cast.lower().split('d')[-1].split('.')[0]\n",
    "        cruise_data_nc.xarray2netcdf_save(xdf = cruise_data_nc.get_xdf(),\n",
    "                                   filename=cruise_name+'c'+cast_label.zfill(3)+'_btl.nc',format=\"NETCDF3_CLASSIC\")\n",
    "\n",
    "  \n",
    "    except:\n",
    "        print(f'Skipping {cast}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-nature",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "QC of data (plot parameters with other instruments)\n",
    "- be sure to updated the qc_status and the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667a4ef3-6642-4a67-86b0-e4cb47f0a6c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:p312]",
   "language": "python",
   "name": "conda-env-p312-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
